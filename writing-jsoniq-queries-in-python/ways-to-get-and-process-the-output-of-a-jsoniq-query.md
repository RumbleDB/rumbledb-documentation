# Ways to get and process the output of a JSONiq query

There are several ways to get back the output of the JSONiq query. There are many examples of use further down this page.

<table><thead><tr><th>Method</th><th>Description</th><th width="135.95703125">Requirement in availableOutputs()</th><th>Scale</th></tr></thead><tbody><tr><td>availableOutputs()</td><td>Returns a list that helps you understand which output methods you can call. The strings in this list can be Local, RDD, DataFrame, or PUL.</td><td></td><td>-</td></tr><tr><td>json()</td><td>Returns the results as a tuple containing dicts, lists, strs, ints, floats, booleans, Nones.</td><td>Local</td><td>Sequence length below the materialization cap. The default is 200 but it can be increased in the RumbleDB configuration.</td></tr><tr><td>df()</td><td>Returns the results as a pyspark data frame</td><td>DataFrame (i.e., RumbleDB was able to infer an output schema)</td><td>No limitation, but beyond a billion items, you should use a Spark cluster.</td></tr><tr><td>pdf()</td><td>Returns the results as a pandas data frame</td><td>DataFrame (i.e., RumbleDB was able to infer an output schema)</td><td>Should fit in your computer's memory.</td></tr><tr><td>rdd()</td><td>Returns the results as an RDD containing dicts, lists, strs, ints, floats, booleans, Nones (experimental)</td><td>RDD</td><td>No limitation, but beyond a billion items, you should use a Spark cluster.</td></tr><tr><td>items()</td><td>Returns the results as a list containing Java Item objects that can be queried with the RumbleDB Item API. Will contain more information and more accurate typing.</td><td>Local</td><td>Sequence length below the materialization cap. The default is 200 but it can be increased in the RumbleDB configuration.</td></tr><tr><td>open(), hasNext(), nextJSON(), close()</td><td>Allows streaming (with no limitation of length) through individuals items as dicts, lists, strs, ints, floats, booleans, Nones.</td><td>Local</td><td>No limitation, as long as you go through the stream without saving all past items.</td></tr><tr><td>open(), hasNext(), next(), close()</td><td>Allows streaming (with no limitation of length) through individuals items as Java Item objects that can be queried with the RumbleDB Item API. Will contain more information and more accurate typing.</td><td>Local</td><td>No limitation, as long as you go through the stream without saving all past items.</td></tr><tr><td>applyUpdates()</td><td>Persists the Pending Update List produced by the query (to the Delta Lake or a table registered in the Hive metastore).</td><td>PUL</td><td>-</td></tr></tbody></table>
